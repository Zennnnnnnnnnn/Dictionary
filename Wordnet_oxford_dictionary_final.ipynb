{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXFEFsi2ug6D"
      },
      "source": [
        "Hướng dẫn sử dụng:\n",
        "\n",
        "Tải Data dạng XML lên GG Colab\n",
        "\n",
        "Chạy lần lượt các phần.\n",
        "\n",
        "Nhập tên Data ở phần 5 thay thế cho /any.xml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-mAD3qKeF7P"
      },
      "source": [
        "Phần 1)   Đo độ tương đồng giữa các định nghĩa tiếng anh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2IKBOt4BRQha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2551f7-e65c-44e3-b811-d14c61acdf03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "# 1) từ 4h40pm ngày 03-09-2024 đưa đo độ tương đồng của Quang vào cùng file với từ điển WORDNET  ----------------------------OK\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Hàm để lấy embeddings từ mô hình GTE-Large\n",
        "def get_embeddings(texts):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\n",
        "    model = AutoModel.from_pretrained(\"thenlper/gte-large\")\n",
        "\n",
        "    # Tokenize the input texts\n",
        "    batch_dict = tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch_dict)\n",
        "\n",
        "    # Average pooling\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    attention_mask = batch_dict['attention_mask']\n",
        "    embeddings = average_pool(last_hidden_states, attention_mask)\n",
        "\n",
        "    # Normalize embeddings\n",
        "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    return embeddings.numpy()\n",
        "\n",
        "# Hàm để thực hiện average pooling trên các embeddings\n",
        "def average_pool(last_hidden_states: torch.Tensor,\n",
        "                 attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "# Hàm để tìm cặp mô tả có độ tương đồng cao\n",
        "def find_high_similarity_pairs(list1, list2, threshold=0.83):\n",
        "\n",
        "    # Trích xuất các đn tiếng anh trong danh sách `list1` và `list2`\n",
        "    descriptions1 = [item['em'] for item in list1] ;\n",
        "    descriptions2 = [item['em'] for item in list2] ;\n",
        "\n",
        "    # Lấy embeddings cho các mô tả trong cả hai danh sách\n",
        "    embeddings1 = get_embeddings(descriptions1) ;\n",
        "    embeddings2 = get_embeddings(descriptions2) ;\n",
        "\n",
        "    # Tính toán ma trận độ tương đồng giữa các embeddings\n",
        "    similarities = cosine_similarity(embeddings1, embeddings2)\n",
        "\n",
        "    # In ma trận độ tương đồng với các giá trị > threshold\n",
        "    filtered_similarities = np.where(similarities > threshold, similarities, 0)\n",
        "\n",
        "    # Khởi tạo danh sách lưu trữ các cặp tương đồng cao\n",
        "    pairs = []\n",
        "\n",
        "    # Tạo hai tập hợp để theo dõi các chỉ số đã được ghép cặp\n",
        "    matched1 = set()\n",
        "    matched2 = set()\n",
        "\n",
        "    # Duyệt qua từng hàng trong ma trận độ tương đồng\n",
        "    for i, row in enumerate(similarities):\n",
        "        # Duyệt qua từng cột trong hàng hiện tại\n",
        "        for j, similarity in enumerate(row):\n",
        "            # Kiểm tra nếu độ tương đồng lớn hơn ngưỡng và cả hai mô tả chưa được ghép cặp\n",
        "            if similarity > threshold and i not in matched1 and j not in matched2:\n",
        "                # Thêm các chỉ số vào các tập hợp đã được ghép cặp\n",
        "                matched1.add(i)\n",
        "                matched2.add(j)\n",
        "                # Thêm cặp vào danh sách các cặp tương đồng cao\n",
        "                pairs.append({\n",
        "                    'Index_List1': i,\n",
        "                    'Index_List2': j,\n",
        "                    'Similarity': similarity\n",
        "                })\n",
        "                # In ra các cặp có độ tương đồng cao nhất\n",
        "                print(f\"Added pair: Index_List1={i}, Index_List2={j}, Similarity={similarity}\")\n",
        "\n",
        "    return pairs    # Trả về danh sách các cặp mô tả có độ tương đồng cao\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-hQNlAWd9RC"
      },
      "source": [
        "Phần 2)  Từ điển EEV Oxford, file XML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6T_6bIhxRHbY"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as et\n",
        "from lxml import etree\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "def wordlist(file_paths):\n",
        "  word_list=[]\n",
        "  for file_path in file_paths:\n",
        "    print(\"File_path = \", file_path)\n",
        "    tree=et.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    for i in root.iter():\n",
        "      if i.tag == \"runhd\":\n",
        "        word_list.append(i.text.strip())\n",
        "  return root, word_list\n",
        "\n",
        "\n",
        "def findword(root, nhap_1_tu):\n",
        "  kq = root.findall(\".//runhd\") ;\n",
        "  for tu in kq:\n",
        "    if tu.text == nhap_1_tu:\n",
        "      break\n",
        "  return tu\n",
        "\n",
        "# hàm tìm cha của 1 node\n",
        "def findfather(root, ptu): ##### thêm đối số root ngày 22-09-24\n",
        "  papa=None\n",
        "  for i in root.iter():\n",
        "    if ptu in i: papa=i; break\n",
        "  return papa\n",
        "\n",
        "def textprocess (chuoi):\n",
        "  newchuoi=chuoi.strip()\n",
        "  kytudb = r'[~@#$%^&*<>?:{}|\\\\\\[\\]]'\n",
        "  newchuoi= re.sub(r'/[^/]+/',\" \", newchuoi)\n",
        "  newchuoi= re.sub(kytudb,\" \",newchuoi)\n",
        "  newchuoi= re.sub(r'\\s{2,}', \" \", newchuoi)\n",
        "  newchuoi=newchuoi.replace(\" ,\" , \",\")\n",
        "  newchuoi=newchuoi.replace(\" , \" , \", \")\n",
        "  newchuoi=newchuoi.replace(\" .\",\".\")\n",
        "  newchuoi=newchuoi.replace(\" . \",\". \")\n",
        "  return newchuoi\n",
        "\n",
        "\n",
        "# Tìm các chuỗi nằm dưới 1 node\n",
        "def alltext_under1node(element):\n",
        "    text_list = []\n",
        "\n",
        "    for elem in element.iter():\n",
        "        if elem.text is not None:\n",
        "            text_list.append(elem.text.strip())\n",
        "        if elem.tail is not None:\n",
        "            text_list.append(elem.tail.strip())\n",
        "\n",
        "    result = ' '.join(text_list)\n",
        "    return result\n",
        "\n",
        "# hàm lọc ra các text định nghĩa tiếng anh\n",
        "def text_outside_children(nodecha):\n",
        "  full_text = ''.join(nodecha.itertext())\n",
        "\n",
        "  symbol=nodecha.find(\".//symbol\")\n",
        "  if symbol is not None:\n",
        "    symbol_texts=''.join(symbol.itertext())  #\n",
        "    full_text=full_text.replace(symbol_texts,\"\").replace(\"\",\"\")\n",
        "\n",
        "  txt_v_s_srf=nodecha.find(\".//txt_v_s_srf\");\n",
        "  if txt_v_s_srf is not None:\n",
        "    txt_v_s_srf_texts=''.join(txt_v_s_srf.itertext()) ;\n",
        "    full_text=full_text.replace(txt_v_s_srf_texts,\"\")\n",
        "\n",
        "  return ' '.join(full_text.split()).strip()\n",
        "\n",
        "def meaningex(root,d_ud): # Thêm đối số root vào 22-09-24 # hàm nhập vào 1 mảng 1 node <d> hay <ud>, trả về nghĩa anh, việt, các ví dụ\n",
        " #print(\" + Nghĩa tiếng anh : \", end=\"\")\n",
        "  em = \"\"\n",
        "  dhs=\"\"\n",
        "  if d_ud.tag in [\"d\", \"ud\"]:\n",
        "    em = text_outside_children(d_ud) ;\n",
        "    if \"\" in em: em = em[0:em.find(\"\")]\n",
        "  elif d_ud.tag in [\"xr\"]:\n",
        "    for dud in d_ud.iter():\n",
        "      text= dud.text.strip() if dud.text else \"\"\n",
        "      tail= dud.tail.strip() if dud.tail else \"\"\n",
        "      #print(text, tail, \" \", end=\"\")\n",
        "      em = em + text + tail\n",
        "    em=em.strip();\n",
        "  #print(\" + nghĩa tiếng anh trong hàm meaningex = \",em) # in nghĩa tiếng anh\n",
        "\n",
        "  # nghĩa tiếng Việt\n",
        "  vm=\"\";\n",
        "  #print(\"\") ;\n",
        "  txt_v_s_srf = d_ud.find(\".//txt_v_s_srf\");\n",
        "  list_z = findfather(root,d_ud).findall(\".//z\");\n",
        "  if txt_v_s_srf is not None:\n",
        "    #print(\" + Nghĩa tiếng Việt: \",end=\"\")\n",
        "    for txt in txt_v_s_srf.iter():\n",
        "      text= txt.text.strip() if txt.text else \"\"\n",
        "      tail= txt.tail.strip() if txt.tail else \"\"\n",
        "      vm = vm + text + tail ;\n",
        "\n",
        "  elif \"\" in d_ud.text:\n",
        "    stt = d_ud.text.find(\"\") ;\n",
        "    vm = vm + d_ud.text[stt:]\n",
        "\n",
        "  #print (\"vm= \", vm)\n",
        "  #print()\n",
        "  vidu=findfather(root,d_ud).findall(\".//x\");\n",
        "  ex=[]\n",
        "  for x in vidu:\n",
        "    kq1=''.join(x.itertext())\n",
        "    kq2=' '.join(kq1.split())\n",
        "    ex.append(kq2 ) ;\n",
        "  #print(\" + Ví dụ ex = \", ex)\n",
        "\n",
        "  return em, vm, ex\n",
        "\n",
        "\n",
        "def dictionary_1word_xml(root,word): # hàm nhận đối số là root của file xml và 1 từ, trả về thông tin là 1 dictionary có các key là phiên âm và các từ loại\n",
        "  thongtin = {}\n",
        "  runhd=findword(root, word)\n",
        "  father = findfather(root,runhd);\n",
        "  grand = findfather(root,father);\n",
        "  greatgrand = findfather(root,grand);\n",
        "\n",
        "  pronunciation=[]\n",
        "  phienam=father.findall(\".//i\");\n",
        "  for j in phienam:\n",
        "    #print(\"phiên âm = /\", j.text,\"/\")\n",
        "    pronunciation.append(j.text)\n",
        "\n",
        "  thongtin[\"pronunciation\"]=pronunciation\n",
        "  p_g=greatgrand.findall(\".//p-g\") ;\n",
        "  if not p_g :\n",
        "    if greatgrand.find(\".//z_p\")is not None:\n",
        "      wordtype = greatgrand.find(\".//z_p\").text.strip()\n",
        "    else: return thongtin\n",
        "    thongtin[wordtype] =  []\n",
        "    n_g = greatgrand.findall(\".//n-g\") ;\n",
        "    if not n_g :\n",
        "      if greatgrand.find(\".//d\") : d_ud = greatgrand.find(\".//d\") ;\n",
        "      elif greatgrand.find(\".//ud\") : d_ud = greatgrand.find(\".//ud\");\n",
        "      elif greatgrand.find(\".//xr\") : d_ud = greatgrand.find(\".//xr\") ;\n",
        "      elif greatgrand.find(\".//h-g\") : d_ud = greatgrand.find(\".//h-g\") ;\n",
        "      if d_ud :\n",
        "        em, vm, ex = meaningex(root, d_ud)\n",
        "        thongtin[wordtype].append({\"em\":em, \"vm\":vm, \"ex\":ex})\n",
        "    else:\n",
        "      for ng  in n_g:\n",
        "        if findfather(root, ng).tag not in ['pv-g','id-g']:\n",
        "          zn=ng.find(\".//zn\")\n",
        "          d_ud= ng.find(\".//d\");\n",
        "          if ng.find(\".//d\") :    d_ud = ng.find(\".//d\");\n",
        "          elif ng.find(\".//ud\") : d_ud = ng.find(\".//ud\");\n",
        "          elif ng.find(\".//xr\") : d_ud = ng.find(\".//xr\");\n",
        "          if d_ud:\n",
        "            em, vm, ex = meaningex( root, d_ud)  ;\n",
        "            thongtin[wordtype].append({\"em\":em, \"vm\":vm, \"ex\":ex})\n",
        "        d_ud=None\n",
        "  else:\n",
        "    for pg in p_g:\n",
        "      z_p_in_p_g = pg.find(\".//z_p_in_p-g\") ;\n",
        "      if z_p_in_p_g is not None:\n",
        "        wordtype = z_p_in_p_g.text.strip()\n",
        "      else: return thongtin\n",
        "      thongtin[wordtype] =  []\n",
        "      n_g = pg.findall(\".//n-g\")\n",
        "      if not n_g :\n",
        "        if pg.find(\".//d\") : d_ud = pg.find(\".//d\");\n",
        "        elif pg.find(\".//ud\") : d_ud = pg.find(\".//ud\");\n",
        "        elif pg.find(\".//xr\") : d_ud = pg.find(\".//xr\")\n",
        "        if d_ud: em, vm,ex = meaningex(root, d_ud) ; thongtin[wordtype].append({\"em\":em, \"vm\":vm, \"ex\": ex})\n",
        "      else:\n",
        "        for ng in n_g:\n",
        "          if findfather(root, ng).tag not in ['pv-g','id-g']:\n",
        "            zn=ng.find(\".//zn\")\n",
        "            if ng.find(\".//d\") : d_ud = ng.find(\".//d\");\n",
        "            elif ng.find(\".//ud\") : d_ud = ng.find(\".//ud\");\n",
        "            elif ng.find(\".//xr\") : d_ud = ng.find(\".//xr\")\n",
        "            if d_ud: em, vm, ex = meaningex(root, d_ud); thongtin[wordtype].append({\"em\":em, \"vm\":vm, \"ex\":ex});\n",
        "          d_ud=None\n",
        "\n",
        "  #print(f\"Thông tin của {word} là = \", thongtin)\n",
        "  return thongtin  # thongtin là 1 đối tượng kiểu dictionary key là phiên âm pronunciation và từ loại wordtype:noun, verb, adj., adv., values của các keys này là nghĩa anh em, nghĩa việt, VD ex\n",
        "\n",
        "\n",
        "# input: 1 từ điển của 1 từ --> output: là từ điển chỉ giữ lại các wordtype-[{\"em\":đn tiếng anh},{\"em: đn tiếng anh\"}, ...]\n",
        "def em_list_dic(tudien1tu):\n",
        "  kq={\"noun\":[], \"verb\":[], \"adj.\":[], \"adv.\":[]}\n",
        "  for word_type, values  in tudien1tu.items():\n",
        "    mang_em=[]\n",
        "    if word_type in [\"pronunciation\",\"synset_id\"]: continue\n",
        "    for i in range(len(values)):\n",
        "      if values[i][\"em\"] :\n",
        "        mang_em.append({\"em\":values[i][\"em\"]})\n",
        "    kq[word_type] = mang_em\n",
        "\n",
        "  # trước khi trả về giá trị kq, xóa đi các từ loại mà không có giá trị\n",
        "  keys_to_remove = []\n",
        "  for key, value in kq.items():\n",
        "    if not value:    keys_to_remove.append(key)\n",
        "  for key in keys_to_remove:  del kq[key]\n",
        "  return kq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtuWhTIbdzgA"
      },
      "source": [
        "Phần 3) Xây dựng từ điển EE từ WordNet   03-09-2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQM7z7FgnJ54",
        "outputId": "1a5e83f2-ff4c-42a8-e08e-2a0eb3dccbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Hàm để tạo từ điển Anh-Anh từ WordNet: nhập vào 1 từ, trả về các cặp key-value là synset_id - {thông tin loại từ, định nghĩa, đồng nghĩa, trái nghĩa, cha, con, VD}\n",
        "def dictionary_1word_wordnet(word):\n",
        "  wordresult = {\"noun\": [], \"verb\": [], \"adj.\": [], \"adv.\": []}\n",
        "  synsets = wordnet.synsets(word);\n",
        "  for synset in synsets:                                       # Duyệt qua tất cả synsets và thu thập thông tin\n",
        "    synset_id = str(synset.name());\n",
        "    wordtype = synset.pos()\n",
        "    if wordtype == 'n' or wordtype == \"s\": wordtype=\"noun\" ;\n",
        "    elif wordtype == 'v': wordtype=\"verb\" ;\n",
        "    elif wordtype == 'a': wordtype=\"adj.\" ;\n",
        "    elif wordtype == 'r': wordtype=\"adv.\" ;\n",
        "\n",
        "    em = synset.definition()\n",
        "    synonyms = [lemma.name() for lemma in synset.lemmas()]\n",
        "    antonyms_kieu_lemma= synset.lemmas()[0].antonyms()\n",
        "    antonyms = [antonym.name() for antonym in antonyms_kieu_lemma]\n",
        "    parents = [str(parent.name()) for parent in synset.hypernyms()]\n",
        "    childrens = [str(child.name()) for child in synset.hyponyms()]\n",
        "    ex = synset.examples()\n",
        "    wordresult[wordtype].append({\"synset_id\": synset_id, \"wordtype\": wordtype,\"em\": em, 'synonyms': synonyms,\"antonyms\": antonyms,'parents':parents, 'children': childrens,\"ex\": ex} ) # Thêm thông tin vào từ điển\n",
        "  keys_to_remove = []\n",
        "  for key, value in wordresult.items():\n",
        "    if not value:    keys_to_remove.append(key)\n",
        "  for key in keys_to_remove:  del wordresult[key]\n",
        "  return wordresult\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phần 4) Tạo từ điển"
      ],
      "metadata": {
        "id": "xpFHEdrIuWdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viết hàm sắp xếp thứ tự mảng các chuỗi theo A,a, B, b...\n",
        "def custom_sort(s):\n",
        "    return (s[0].lower(), not s[0].isupper())\n",
        "\n",
        "\n",
        "# Sắp xếp 1 mảng các từ tiếng anh, trả về theo thứ tự alphabe như trong mảng TT bên dưới, TT có ký tự khoảng trắng nằm cuối cùng\n",
        "def compare_words(word1, word2,TT = '\\'_-./ 0123456789AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'):\n",
        "    min_length = min(len(word1), len(word2))\n",
        "    for i in range(min_length):\n",
        "        index1 = TT.index(word1[i])\n",
        "        index2 = TT.index(word2[i])\n",
        "        if index1 < index2:\n",
        "            return -1\n",
        "        elif index1 > index2:\n",
        "            return 1\n",
        "\n",
        "    # Nếu đến đây, hai từ có cùng phần đầu, so sánh theo độ dài\n",
        "    if len(word1) < len(word2):\n",
        "        return -1\n",
        "    elif len(word1) > len(word2):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def bubble_sort(words): # nhập vào mảng các từ, trả về mảng đã được sắp xếp\n",
        "    n = len(words)\n",
        "    for i in range(n - 1):\n",
        "        for j in range(n - i - 1):\n",
        "            if compare_words(words[j], words[j + 1]) > 0:\n",
        "                words[j], words[j + 1] = words[j + 1], words[j]\n",
        "    return words\n",
        "\n",
        "\n",
        "def quick_sort(words, compare_func):\n",
        "    def partition(low, high):\n",
        "        pivot = words[high]\n",
        "        i = low - 1\n",
        "        for j in range(low, high):\n",
        "            if compare_func(words[j], pivot) <= 0:\n",
        "                i += 1\n",
        "                words[i], words[j] = words[j], words[i]\n",
        "        words[i + 1], words[high] = words[high], words[i + 1]\n",
        "        return i + 1\n",
        "\n",
        "    def quick_sort_recursive(low, high):\n",
        "        if low < high:\n",
        "            pi = partition(low, high)\n",
        "            quick_sort_recursive(low, pi - 1)\n",
        "            quick_sort_recursive(pi + 1, high)\n",
        "\n",
        "    quick_sort_recursive(0, len(words) - 1)\n",
        "    return words\n",
        "\n",
        "\n",
        "def count_wordnet(): #TRẢ về có bao nhiêu phần tử bắt đầu bằng ký tự kytu trong ds tat cac cac tu cua wordnet, Trả về vị trí bắt đầu và vị trí kết thúc của các phần tử bắt đầu cùa từng vần A, B...\n",
        "  all_words_set = set()\n",
        "  for synset in wordnet.all_synsets():\n",
        "    for lemma in synset.lemmas():\n",
        "      all_words_set.add(lemma.name())\n",
        "  all_words_list = quick_sort(list(all_words_set), compare_words ) # sắp xếp A, a, B, b, ...\n",
        "\n",
        "  chucaihoa=[\"A\",\"B\",\"C\",\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
        "  chucaithuong= [ \"a\", \"b\" , \"c\" , \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"J\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
        "  ketquadem=[0]*27            # Tạo mảng 27 phần tử có giá trị ban đầu là 0\n",
        "  vitribd=[0]*27\n",
        "  vitrikt=[0]*27\n",
        "\n",
        "   # Đếm số lượng phần tử bắt đầu bằng chữ 'A', 'B', ...\n",
        "  for i in range(len(chucaihoa)):\n",
        "    kytu=chucaihoa[i]\n",
        "    for j in all_words_set:\n",
        "      if j.startswith(kytu) == True :  ketquadem[i]=ketquadem[i]+1\n",
        "\n",
        "  for i in range(len(chucaithuong)):\n",
        "    kytu=chucaithuong[i]\n",
        "    for j in all_words_set:\n",
        "      if j.startswith(kytu) == True :  ketquadem[i]=ketquadem[i]+1\n",
        "\n",
        "  ketquadem[i+1] = len(all_words_set) - sum(ketquadem)\n",
        "\n",
        "  # Tìm phần tử đầu tiên bắt đầu bằng ký tự target_char\n",
        "  def FindFchar(array, char):\n",
        "    for item in array:\n",
        "        if item.startswith(char):\n",
        "            return item, array.index(item)\n",
        "    return None\n",
        "\n",
        "  for k  in range(len(chucaihoa)):\n",
        "    for f  in range(len(all_words_list)):\n",
        "      if all_words_list[f].startswith(chucaihoa[k]):\n",
        "        vitribd[k] = f\n",
        "        vitrikt[k] = f + ketquadem[k] #- 1\n",
        "        break\n",
        "\n",
        "  vitribd[26] = 0\n",
        "  vitrikt[26] = vitribd[0]-1\n",
        "\n",
        "  return all_words_list, len(all_words_list), ketquadem, vitribd, vitrikt\n",
        "\n",
        "\n",
        "def dictionary_wordnet(fr, to):                # nhập vào giá trị từ số --> tới số , trả về từ điển wordnet gồm tất cả các từ\n",
        "  all_words_set = set()\n",
        "  for synset in wordnet.all_synsets():\n",
        "    for lemma in synset.lemmas():\n",
        "      all_words_set.add(lemma.name())\n",
        "  all_words_list = quick_sort(list(all_words_set), compare_words)\n",
        "\n",
        "  # Lấy 1 vài từ trong danh sách,\n",
        "  words = all_words_list[fr:to]    ;\n",
        "  EE = {}\n",
        "  for word in words:\n",
        "    EE[word] = dictionary_1word_wordnet(word)    # thêm từ điển của 1 từ vào EEV\n",
        "  return EE\n",
        "\n",
        "# --------------------------------------- Nhập đường dẫn File Oxford vào -------------------------------------------------------------------------\n",
        "\n",
        "#Hàm nhập vào 1 file từ điển xml của oxford, trả về từ điển tích hợp wordnet và xml\n",
        "def intergration_dic(file_paths=[\"/any.xml\"]):\n",
        "  #file_paths = [\"/any.xml\"]\n",
        "  root_xml, word_list_xml = wordlist(file_paths)  # hàm trả về root và danh sách các từ trong file xml\n",
        "  #print(\"root_xml = \" , root_xml)\n",
        "\n",
        "  # lấy thông tin của từ điển WORDNET\n",
        "  word_list_wn, soluongtu, ketquadem, vitribd, vitrikt=count_wordnet() ;\n",
        "\n",
        "  # Xác định vitribd và vitrikt dựa vào từ đầu tiên của file xml\n",
        "  chucaihoa=[\"A\",\"B\",\"C\",\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
        "  vitri=chucaihoa.index(word_list_xml[0][0])\n",
        "  EE_any_wn=dictionary_wordnet(vitribd[vitri], vitrikt[vitri])\n",
        "\n",
        "  # Tìm các từ xml có trong wordnet\n",
        "  result = [word for word in word_list_xml if word in word_list_wn]\n",
        "\n",
        "  word_list_wn = result   #############################################################################\n",
        "\n",
        "  # xét từng từ của wordnet và tìm thongtin_tudien của từ này trong Oxford XML\n",
        "  for word_wn in word_list_wn:\n",
        "    print(\"word is: \", word_wn)\n",
        "    thongtin1tu_wn=dictionary_1word_wordnet(word_wn) ;\n",
        "    em_list_wn = em_list_dic(thongtin1tu_wn)\n",
        "    if word_wn in word_list_xml:\n",
        "      thongtin1tu_xml=dictionary_1word_xml(root_xml, word_wn) ;\n",
        "      em_list_xml = em_list_dic(thongtin1tu_xml)  ;\n",
        "      EE_any_wn[word_wn][\"pronunciation\"] = thongtin1tu_xml[\"pronunciation\"]\n",
        "\n",
        "  # so sánh độ tương đồng 2 mảng định nghĩa tiếng anh\n",
        "    for loaitu1, em_dn1 in em_list_wn.items():\n",
        "      wn = em_list_wn[loaitu1]       ;\n",
        "      for loaitu2, em_dn2 in em_list_xml.items():\n",
        "        xml= em_list_xml[loaitu2]\n",
        "        if loaitu1 == loaitu2:    # nếu cả 2 cùng từ loại noun, hoặc verb, ...\n",
        "          mangdotd=find_high_similarity_pairs(wn, xml, threshold=0.83)\n",
        "          for dotd in mangdotd:\n",
        "              EE_any_wn[word_wn][loaitu1][dotd[\"Index_List1\"]][\"em_ox\"] = thongtin1tu_xml[loaitu2][dotd[\"Index_List2\"]][\"em\"]\n",
        "              EE_any_wn[word_wn][loaitu1][dotd[\"Index_List1\"]][\"vm_ox\"] = thongtin1tu_xml[loaitu2][dotd[\"Index_List2\"]][\"vm\"]\n",
        "              EE_any_wn[word_wn][loaitu1][dotd[\"Index_List1\"]][\"ex_ox\"] = thongtin1tu_xml[loaitu2][dotd[\"Index_List2\"]][\"ex\"]\n",
        "          break\n",
        "\n",
        "  # Tạo 1 file sẽ lưu kết quả cuối cùng của từ điển tích hợp wordnet và oxford\n",
        "  with open('/finaldic_any.json', 'w') as file:\n",
        "    file.write('{}')\n",
        "\n",
        " # Lưu kết quả vào file finadic_any.json\n",
        "  with open(\"/finaldic_any.json\", 'w', encoding='utf-8') as file:\n",
        "    json.dump(EE_any_wn, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "  duong_dan= \"/finaldic_any.json\"\n",
        "  #duong_dan=os.path.join(os.getcwd(), \"finaldic_any.json\")\n",
        "  return duong_dan\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RcJBEsGWuUxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phần 5) Nhập thông tin"
      ],
      "metadata": {
        "id": "rTklHKISo2Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- người dùng Chỉ cần nhập đường dẫn vào thế chỗ \"/a.xml\", kết quả trả về là đường dẫn lưu file \"/finaldic_any.json\" -----------------------\n",
        "duongdankq=intergration_dic([\"/any.xml\"])\n",
        "print(\" ***** Đường dẫn lưu kết quả là = \",duongdankq)\n"
      ],
      "metadata": {
        "id": "As2P2vibou1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGLA6hrKSLay"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phần 6) Tạo từ điển dạng csv và excel"
      ],
      "metadata": {
        "id": "IoXy03c3ucYW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYMYRUkXxN05"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Đọc file JSON\n",
        "with open('finaldic_any.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Chuẩn bị một DataFrame rỗng để lưu trữ tất cả dữ liệu\n",
        "all_data = []\n",
        "\n",
        "# Xử lý từng mục trong dữ liệu JSON\n",
        "for word, details in data.items():\n",
        "    # Lấy phần phát âm\n",
        "    pronunciation = details.get(\"pronunciation\", [])\n",
        "    pronunciation_value = pronunciation[0] if pronunciation else \"\"\n",
        "\n",
        "    # Lặp qua danh sách danh từ\n",
        "    for noun in details.get(\"noun\", []):\n",
        "        noun['word'] = word  # Thêm từ vào từng mục\n",
        "        noun['pronunciation'] = pronunciation_value  # Thêm phát âm\n",
        "\n",
        "        # Thêm các trường bổ sung nếu có\n",
        "        noun['em_ox'] = noun.get('em_ox', \"\")\n",
        "        noun['vm_ox'] = noun.get('vm_ox', \"\")\n",
        "        noun['ex_ox'] = noun.get('ex_ox', [])\n",
        "\n",
        "        all_data.append(noun)\n",
        "\n",
        "# Chuyển đổi dữ liệu thành DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Sắp xếp lại cột: 'word', 'pronunciation', 'synset_id', 'wordtype', v.v.\n",
        "df = df[['word', 'pronunciation', 'synset_id', 'wordtype', 'em', 'synonyms', 'antonyms', 'parents', 'children', 'ex', 'em_ox', 'vm_ox', 'ex_ox']]\n",
        "\n",
        "# Lưu thành file CSV\n",
        "df.to_csv('finaldic_any.csv', index=False)\n",
        "\n",
        "# Lưu thành file Excel\n",
        "df.to_excel('finaldic_any.xlsx', index=False, engine='openpyxl')\n",
        "\n",
        "print(\"Files have been created: data.csv and data.xlsx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}